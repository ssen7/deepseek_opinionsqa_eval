{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e40b34bd",
   "metadata": {},
   "source": [
    "# 1. LOAD DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "619da30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac05e38317d6472fb1a970e9d833cab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ref: https://gist.github.com/vgel/8a2497dc45b1ded33287fa7bb6cc1adc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache\n",
    "import torch\n",
    "\n",
    "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model_name='deepseek-ai/DeepSeek-R1-Distill-Qwen-32B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('/scratch/ss4yd/huggingfacemodels/DeepSeek-R1-Distill-Qwen-32B/', \n",
    "                                             torch_dtype=torch.bfloat16, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efdde8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>options</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please think about what things will be like in...</td>\n",
       "      <td>['Very optimistic', 'Somewhat optimistic', \\r\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Over the next 30 years, do you think that the ...</td>\n",
       "      <td>['Get better', 'Stay about the same', \\r\\n'Get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thinking about how Chinese society sees men th...</td>\n",
       "      <td>['Mostpeoplelook uptomenwhoaremanlyormasculine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Over the next 30 years, do you think that crim...</td>\n",
       "      <td>['Get better', 'Stay about the same', \\r\\n'Get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Over the next 30 years, do you think that yout...</td>\n",
       "      <td>['Get better', 'Stay about the same', \\r\\n'Get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Over the next 30 years, do you think that Chin...</td>\n",
       "      <td>['Get better', 'Stay about the same', \\r\\n'Get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Over the next 30 years, do you think that Chin...</td>\n",
       "      <td>['Get better', 'Stay about the same', \\r\\n'Get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Over the next 30 years, do you think that soci...</td>\n",
       "      <td>['Get better', 'Stay about the same', \\r\\n'Get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Do you think China's current economic conditio...</td>\n",
       "      <td>['Helping a lot','Helping a little', 'Hurting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Do you think China's current economic conditio...</td>\n",
       "      <td>['Helping a lot','Helping a little', 'Hurting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Do you think China's current economic conditio...</td>\n",
       "      <td>['Helping a lot','Helping a little', 'Hurting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Do you think China's current economic conditio...</td>\n",
       "      <td>['Helping a lot','Helping a little', 'Hurting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>When it comes to whether or not to limit Chine...</td>\n",
       "      <td>['Strongly support limiting Chinese students',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>When it comes to whether or not to limit Ameri...</td>\n",
       "      <td>['Strongly support limiting American students'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Do you think independent civil society is gene...</td>\n",
       "      <td>['Very good for society','Somewhat good for so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Do you think protests are generally good or ba...</td>\n",
       "      <td>['Very good for society','Somewhat good for so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   Please think about what things will be like in...   \n",
       "1   Over the next 30 years, do you think that the ...   \n",
       "2   Thinking about how Chinese society sees men th...   \n",
       "3   Over the next 30 years, do you think that crim...   \n",
       "4   Over the next 30 years, do you think that yout...   \n",
       "5   Over the next 30 years, do you think that Chin...   \n",
       "6   Over the next 30 years, do you think that Chin...   \n",
       "7   Over the next 30 years, do you think that soci...   \n",
       "8   Do you think China's current economic conditio...   \n",
       "9   Do you think China's current economic conditio...   \n",
       "10  Do you think China's current economic conditio...   \n",
       "11  Do you think China's current economic conditio...   \n",
       "12  When it comes to whether or not to limit Chine...   \n",
       "13  When it comes to whether or not to limit Ameri...   \n",
       "14  Do you think independent civil society is gene...   \n",
       "15  Do you think protests are generally good or ba...   \n",
       "\n",
       "                                              options  \n",
       "0   ['Very optimistic', 'Somewhat optimistic', \\r\\...  \n",
       "1   ['Get better', 'Stay about the same', \\r\\n'Get...  \n",
       "2   ['Mostpeoplelook uptomenwhoaremanlyormasculine...  \n",
       "3   ['Get better', 'Stay about the same', \\r\\n'Get...  \n",
       "4   ['Get better', 'Stay about the same', \\r\\n'Get...  \n",
       "5   ['Get better', 'Stay about the same', \\r\\n'Get...  \n",
       "6   ['Get better', 'Stay about the same', \\r\\n'Get...  \n",
       "7   ['Get better', 'Stay about the same', \\r\\n'Get...  \n",
       "8   ['Helping a lot','Helping a little', 'Hurting ...  \n",
       "9   ['Helping a lot','Helping a little', 'Hurting ...  \n",
       "10  ['Helping a lot','Helping a little', 'Hurting ...  \n",
       "11  ['Helping a lot','Helping a little', 'Hurting ...  \n",
       "12  ['Strongly support limiting Chinese students',...  \n",
       "13  ['Strongly support limiting American students'...  \n",
       "14  ['Very good for society','Somewhat good for so...  \n",
       "15  ['Very good for society','Somewhat good for so...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('./data/model_input/UVA_Data.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aaac81",
   "metadata": {},
   "source": [
    "# 2. Generate Prompt from Opinions QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f978705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: default \n",
      "Survey file: UVA_Data\n",
      "\n",
      "Generated prompt:\n",
      "Question: Over the next 30 years, do you think that the average Chinese family will see \r\n",
      "its standard of living\n",
      "A. Get better\n",
      "B. Stay about the same\n",
      "C. Get worse\n",
      "D. Refused\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from process_opinions_qa import generate_prompt\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "# read yaml file\n",
    "# change survey_type and context in config.yaml file for different types of prompts as described in the paper\n",
    "with open('config.yaml') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "manual=True # use config file if False, set manually if True\n",
    "if manual:\n",
    "    config=dict()\n",
    "    config['prompt']=dict()\n",
    "    config['prompt']['context']='default' # values: default, steer-qa, steer-bio, steer-portray\n",
    "    config['prompt']['survey_type']='UVA_Data' # values: Pew_American_Trends_Panel_disagreement_500,Pew_American_Trends_Panel_W26, replace 26 with the [26,27,29,32..] etc.\n",
    "    config['prompt']['output_path']='./'\n",
    "    config['prompt']['include_output']=False\n",
    "    config['prompt']['reference_index']=None\n",
    "\n",
    "print(f\"Context: {config['prompt']['context']} \\nSurvey file: {config['prompt']['survey_type']}\")\n",
    "\n",
    "train, evals,_=generate_prompt(config)\n",
    "\n",
    "# print(train[0])\n",
    "\n",
    "# while True:\n",
    "if len(train)>0:\n",
    "    prompt=train[random.choice(range(len(train)))]+'\\n'+evals[random.choice(range(len(evals)))]\n",
    "else:\n",
    "    prompt=evals[random.choice(range(len(evals)))]\n",
    "\n",
    "#     if 'Xi' in prompt:\n",
    "#         break\n",
    "    \n",
    "\n",
    "print('\\nGenerated prompt:')\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecb151c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Question: Please think about what things will be like in 2050, about 30 years from now. \\r\\nThinking about the future of the People's Republic of China, would you say you are\\nA. Very optimistic\\nB. Somewhat optimistic\\nC. Somewhat pessimistic\\nD. Very pessimistic\\nE. Refused\\nAnswer:\",\n",
       " 'Question: Over the next 30 years, do you think that the average Chinese family will see \\r\\nits standard of living\\nA. Get better\\nB. Stay about the same\\nC. Get worse\\nD. Refused\\nAnswer:',\n",
       " 'Question: Thinking about how Chinese society sees men these days, in general, would you say\\nA. Mostpeoplelook uptomenwhoaremanlyormasculine\\nB. Most people look down on men who are manly or masculine\\nC. Neither\\nD. Refused\\nAnswer:',\n",
       " 'Question: Over the next 30 years, do you think that crime rates in Chinese cities will \\nA. Get better\\nB. Stay about the same\\nC. Get worse\\nD. Refused\\nAnswer:',\n",
       " 'Question: Over the next 30 years, do you think that youth unemployment in China will\\nA. Get better\\nB. Stay about the same\\nC. Get worse\\nD. Refused\\nAnswer:',\n",
       " 'Question: Over the next 30 years, do you think that Chinese demographic problems will\\nA. Get better\\nB. Stay about the same\\nC. Get worse\\nD. Refused\\nAnswer:',\n",
       " 'Question: Over the next 30 years, do you think that Chinese birthrates will \\nA. Get better\\nB. Stay about the same\\nC. Get worse\\nD. Refused\\nAnswer:',\n",
       " 'Question: Over the next 30 years, do you think that social stability in China will\\nA. Get better\\nB. Stay about the same\\nC. Get worse\\nD. Refused\\nAnswer:',\n",
       " \"Question: Do you think China's current economic conditions are helping or hurting Han people?\\nA. Helping a lot\\nB. Helping a little\\nC. Hurting a little\\nD. Hurting a lot\\nE. Neither helping nor hurting\\nF. Refused\\nAnswer:\",\n",
       " \"Question: Do you think China's current economic conditions are helping or hurting Uyghur people?\\nA. Helping a lot\\nB. Helping a little\\nC. Hurting a little\\nD. Hurting a lot\\nE. Neither helping nor hurting\\nF. Refused\\nAnswer:\",\n",
       " \"Question: Do you think China's current economic conditions are helping or hurting Tibetan people?\\nA. Helping a lot\\nB. Helping a little\\nC. Hurting a little\\nD. Hurting a lot\\nE. Neither helping nor hurting\\nF. Refused\\nAnswer:\",\n",
       " \"Question: Do you think China's current economic conditions are helping or hurting Hui people?\\nA. Helping a lot\\nB. Helping a little\\nC. Hurting a little\\nD. Hurting a lot\\nE. Neither helping nor hurting\\nF. Refused\\nAnswer:\",\n",
       " 'Question: When it comes to whether or not to limit Chinese students studying in the U.S., do you \\nA. Strongly support limiting Chinese students\\nB. Somewhat support limiting Chinese students\\nC. Somewhat oppose limiting Chinese students\\nD. Strongly oppose limiting Chinese students\\nE. Refused\\nAnswer:',\n",
       " 'Question: When it comes to whether or not to limit American students studying in China, do you \\nA. Strongly support limiting American students\\nB. Somewhat support limiting American students\\nC. Somewhat oppose limiting American students\\nD. Strongly oppose limiting American students\\nE. Refused\\nAnswer:',\n",
       " 'Question: Do you think independent civil society is generally good or bad for our country?\\nA. Very good for society\\nB. Somewhat good for society\\nC. Neither good nor bad for society\\nD. Somewhat bad for society\\nE. Very bad for society\\nF. Refused\\nAnswer:',\n",
       " 'Question: Do you think protests are generally good or bad for our society?\\nA. Very good for society\\nB. Somewhat good for society\\nC. Neither good nor bad for society\\nD. Somewhat bad for society\\nE. Very bad for society\\nF. Refused\\nAnswer:']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908e79b0",
   "metadata": {},
   "source": [
    "# 3. Prompt model with chain-of-thought prompting\n",
    "\n",
    "Taken from: https://gist.github.com/vgel/8a2497dc45b1ded33287fa7bb6cc1adc\n",
    "\n",
    "Code forces the model to generate at least 128 tokens before stopping. The code also used replacement tokens to bridge this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24a4ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://gist.github.com/vgel/8a2497dc45b1ded33287fa7bb6cc1adc\n",
    "\n",
    "_, _start_think_token, end_think_token = tokenizer.encode(\"<think></think>\")\n",
    "\n",
    "think_responses=list()\n",
    "replacements=[\"\\nWait, but\", \"\\nHmm\", \"\\nSo\"]\n",
    "@torch.inference_mode\n",
    "def reasoning_effort(question: str, min_thinking_tokens: int):\n",
    "    tokens = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": \"<think>\\n\" + \"\"},\n",
    "        ],\n",
    "        continue_final_message=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokens = tokens.to(model.device)\n",
    "    kv = DynamicCache()\n",
    "    n_thinking_tokens = 0\n",
    "\n",
    "    print(tokenizer.decode(list(tokens[0])))\n",
    "    while True:\n",
    "        out = model(input_ids=tokens, past_key_values=kv, use_cache=True)\n",
    "        next_token = torch.multinomial(\n",
    "            torch.softmax(out.logits[0, -1, :], dim=-1), 1\n",
    "        ).item()\n",
    "        kv = out.past_key_values\n",
    "\n",
    "        if (\n",
    "            next_token in (end_think_token, model.config.eos_token_id)\n",
    "            and n_thinking_tokens < min_thinking_tokens\n",
    "        ):\n",
    "            replacement = random.choice(replacements)\n",
    "            print(replacement)\n",
    "            replacement_tokens = tokenizer.encode(replacement)\n",
    "            n_thinking_tokens += len(replacement_tokens)\n",
    "            tokens = torch.tensor([replacement_tokens]).to(tokens.device)\n",
    "        elif next_token == model.config.eos_token_id:\n",
    "            break\n",
    "        else:\n",
    "            dec_token = tokenizer.decode([next_token])\n",
    "            think_responses.append(dec_token)\n",
    "            yield tokenizer.decode([next_token])\n",
    "            n_thinking_tokens += 1\n",
    "            tokens = torch.tensor([[next_token]]).to(tokens.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d2f0b",
   "metadata": {},
   "source": [
    "## 3a. Run model with CoT prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c312a5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>Question: Over the next 30 years, do you think that the average Chinese family will see \n",
      "its standard of living\n",
      "A. Get better\n",
      "B. Stay about the same\n",
      "C. Get worse\n",
      "D. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n",
      "Okay, so I have this question about whether the average Chinese family's standard of living will get better, stay the same, get worse, or if the person refuses to answer over the next 30 years. The answer given is A. Get better, but I need to think through why that is the case.\n",
      "\n",
      "First, I'll consider the economic growth in China. They've had remarkable growth over the past few decades, lifting millions out of poverty. The government has invested in infrastructure, technology, and industry, which should continue to drive growth. If the economy keeps expanding, people's incomes are likely to rise, leading to a better standard of living.\n",
      "\n",
      "Then there's technology and innovation. China is investing heavily in tech sectors like AI, 5G, and renewable energy. These advancements can boost productivity and create new job opportunities, further improving living standards.\n",
      "\n",
      "Urbanization is another factor. As more people move to cities, there's better access to services, education, and healthcare. Improved infrastructure in urban areas can enhance the quality of life for many Chinese families.\n",
      "\n",
      "However, I shouldn't ignore potential challenges. Environmental issues, income inequality, and an aging population are significant concerns. If these issues aren't managed well, they could slow down progress. But the Chinese government has been taking steps to address them, such as implementing pollution controls and expanding social safety nets.\n",
      "\n",
      "On the international front, trade relationships and geopolitical tensions could impact China's economy. If they can maintain stable trade relations and navigate global challenges, growth is more likely to continue.\n",
      "\n",
      "Looking at the demographic shifts, an aging population might strain social services, but as the workforce becomes more skilled, there could be increased productivity to offset these challenges.\n",
      "\n",
      "So, weighing all these factors, while there are challenges, the overall trend indicates that with proper management, the standard of living for the average Chinese family is expected to improve over the next 30 years.\n",
      "</think>\n",
      "\n",
      "Over the next 30 years, the average Chinese family's standard of living is expected to improve. This conclusion is supported by several key factors:\n",
      "\n",
      "1. **Economic Growth**: China's sustained economic expansion is a cornerstone of its progress. The government's investments in infrastructure and industry will likely maintain growth momentum, leading to higher incomes and improved living standards.\n",
      "\n",
      "2. **Technological Advancements**: Heavy investments in AI, 5G, and renewable energy are poised to enhance productivity and create new opportunities, further driving economic growth and quality of life improvements.\n",
      "\n",
      "3. **Urbanization**: As urbanization progresses, access to better services, education, and healthcare in cities will enhance living conditions for many families.\n",
      "\n",
      "4. **Government Initiatives**: The Chinese government is addressing challenges such as environmental issues and income inequality through targeted policies, including pollution controls and social safety net expansions.\n",
      "\n",
      "5. **International Relations**: Navigating geopolitical tensions and maintaining stable trade relations will support continued economic growth.\n",
      "\n",
      "6. **Demographic Adaptations**: While an aging population presents challenges, the increasing skill level of the workforce can mitigate these issues, fostering productivity.\n",
      "\n",
      "In conclusion, despite challenges, the strategic management of these factors points towards an overall improvement in the standard of living for the average Chinese family over the next three decades."
     ]
    }
   ],
   "source": [
    "for chunk in reasoning_effort(prompt, 128):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b330f",
   "metadata": {},
   "source": [
    "# 3b Run model without CoT prompting\n",
    "\n",
    "This code was adapted from https://github.com/stanford-crfm/helm cited in the opinions-qa github repo to reproduce their results. It forces the model to generate only one token, which is usually one of the options presented in the prompt.\n",
    "\n",
    "Ref:https://github.com/stanford-crfm/helm/blob/main/src/helm/clients/huggingface_client.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0b66e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_criteria = None\n",
    "raw_request={\n",
    "    \"prompt\":prompt,\n",
    "    \"stop_sequences\": [],\n",
    "    \"temperature\":1e-7,\n",
    "    \"max_new_tokens\":1,\n",
    "    \"top_p\":1,\n",
    "    \"num_return_sequences\":1,\n",
    "    \"echo_prompt\":False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2da0fa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: Below you will be asked to provide a short description of the region you currently reside in within the US and then answer some questions.\n",
      "\n",
      "Description: I currently reside in the Northeast.\n",
      "\n",
      "Question: How much confidence do you have in chinese President Xi Jinping to do the right thing regarding world affairs?\n",
      "\n",
      "A. A lot of confidence\n",
      "B. Some confidence\n",
      "C. Not too much confidence\n",
      "D. No confidence at all\n",
      "E. Refused\n",
      "Answer: B\n",
      "Log Probs: [[-1.042975902557373]]\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(raw_request[\"prompt\"], return_tensors=\"pt\", return_token_type_ids=False).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "                    **encoded_input,\n",
    "    #                 temperature=raw_request[\"temperature\"],\n",
    "                    num_return_sequences=raw_request[\"num_return_sequences\"],\n",
    "                    max_new_tokens=raw_request[\"max_new_tokens\"],\n",
    "                    top_p=raw_request[\"top_p\"],\n",
    "                    do_sample=False,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True,\n",
    "                    stopping_criteria=stopping_criteria,\n",
    "    #                 top_k=raw_request[\"top_k\"],\n",
    "                )\n",
    "sequences = output.sequences\n",
    "scores = output.scores\n",
    "\n",
    "print(tokenizer.decode(sequences[0]))\n",
    "\n",
    "# calculate log probs when scores do not return inf\n",
    "if torch.isinf(scores[0]).sum().item()==0:\n",
    "\n",
    "    # Compute logprobs of generated tokens for each completed sequence.\n",
    "    all_generated_tokens_logprobs = []\n",
    "    for completion_id in range(raw_request[\"num_return_sequences\"]):\n",
    "#         print(f'completion id: {completion_id}')\n",
    "        generated_tokens_logprobs = []\n",
    "        for i in range(len(sequences[completion_id]) - len(encoded_input.input_ids[0])):\n",
    "#             print(f'i: {i}')\n",
    "            logprobs = torch.nn.functional.log_softmax(scores[i][completion_id], dim=0)\n",
    "#             print(f'logprobs: {len(logprobs)}')\n",
    "            # Get log probability of chosen token.\n",
    "            j = i + len(encoded_input.input_ids[0])\n",
    "#             print(f'j: {j}')\n",
    "            generated_tokens_logprobs.append(logprobs[sequences[completion_id][j]].item())\n",
    "        all_generated_tokens_logprobs.append(generated_tokens_logprobs)\n",
    "\n",
    "    print(f'Log Probs: {all_generated_tokens_logprobs}')\n",
    "else:\n",
    "    print('Log Probs not calculated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "93a2e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove prompt from the start of each sequence if echo_prompt is False.\n",
    "# if not raw_request[\"echo_prompt\"]:\n",
    "#     sequences = [sequence[len(encoded_input.input_ids[0]) :] for sequence in sequences]\n",
    "\n",
    "# all_tokens = [[tokenizer.decode(token) for token in sequence_tokens] for sequence_tokens in sequences]\n",
    "# all_decoded_text = tokenizer.batch_decode(sequences)\n",
    "\n",
    "# all_decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbceef19",
   "metadata": {},
   "source": [
    "### Get Logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3e63518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_logprob_dictionary(logprobs, tokenizer, k=10):\n",
    "    res_dict=dict()\n",
    "    topklogprobs=logprobs.topk(k=k)\n",
    "    logprob_vals=topklogprobs.values\n",
    "    logprob_inds=topklogprobs.indices\n",
    "    \n",
    "    res_dict['text']=tokenizer.decode([logprob_inds[0].item()])\n",
    "    res_dict['logprob']=logprob_vals[0].item()\n",
    "    res_dict['top_logprobs']=dict()\n",
    "    for i in range(k):\n",
    "        res_dict['top_logprobs'][tokenizer.decode([logprob_inds[i].item()])]= \\\n",
    "            logprob_vals[i].item()\n",
    "        \n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "85001aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' B',\n",
       " 'logprob': -1.042975902557373,\n",
       " 'top_logprobs': {' B': -1.042975902557373,\n",
       "  ' A': -1.167975902557373,\n",
       "  ' C': -2.042975902557373,\n",
       "  ' \\n\\n': -2.667975902557373,\n",
       "  ' I': -3.605475902557373,\n",
       "  ' D': -4.042975902557373,\n",
       "  ' E': -4.105475902557373,\n",
       "  ' \\n': -4.667975902557373,\n",
       "  ' The': -4.730475902557373,\n",
       "  ' Not': -4.855475902557373}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_logprob_dictionary(logprobs, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea7689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ebbe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e49b0b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ff38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95eb76f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62250c72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc927b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
