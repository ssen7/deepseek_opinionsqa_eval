{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e40b34bd",
   "metadata": {},
   "source": [
    "# 1. LOAD DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "619da30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47a6feccd6242f6b5bd0f2c1f144579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ref: https://gist.github.com/vgel/8a2497dc45b1ded33287fa7bb6cc1adc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache\n",
    "import torch\n",
    "\n",
    "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model_name='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('/scratch/ss4yd/huggingfacemodels/DeepSeek-R1-Distill-Qwen-7B/', \n",
    "                                             torch_dtype=torch.bfloat16, device_map='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aaac81",
   "metadata": {},
   "source": [
    "# 2. Generate Prompt from Opinions QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f978705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: steer-bio \n",
      "Survey file: Pew_American_Trends_Panel_disagreement_500\n",
      "\n",
      "Generated prompt:\n",
      "Question: What is your present religion, if any?\n",
      "A. Protestant\n",
      "B. Roman Catholic\n",
      "C. Mormon\n",
      "D. Orthodox\n",
      "E. Jewish\n",
      "F. Muslim\n",
      "G. Buddhist\n",
      "H. Hindu\n",
      "I. Atheist\n",
      "J. Agnostic\n",
      "K. Other\n",
      "L. Nothing in particular\n",
      "M. Refused\n",
      "Answer: F\n",
      "\n",
      "Question: Since President Trump was elected, do you think it has become more acceptable or less acceptable for people to express racist or racially insensitive views, or is it about as acceptable as it was before?\n",
      "A. More acceptable\n",
      "B. Less acceptable\n",
      "C. About as acceptable\n",
      "D. Refused\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from process_opinions_qa import generate_prompt\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "# read yaml file\n",
    "# change survey_type and context in config.yaml file for different types of prompts as described in the paper\n",
    "with open('config.yaml') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "print(f\"Context: {config['prompt']['context']} \\nSurvey file: {config['prompt']['survey_type']}\")\n",
    "\n",
    "manual=True # use config file if False, set manually if True\n",
    "if manual:\n",
    "    config=dict()\n",
    "    config['prompt']=dict()\n",
    "    config['prompt']['context']='steer-qa' # values: default, steer-qa, steer-bio, steer-portray\n",
    "    config['prompt']['survey_type']='Pew_American_Trends_Panel_disagreement_500' # values: Pew_American_Trends_Panel_W26, replace 26 with the [27,27,29,32..] etc.\n",
    "    config['prompt']['output_path']='./'\n",
    "    config['prompt']['include_output']=False\n",
    "    config['prompt']['reference_index']=None\n",
    "    \n",
    "train, evals=generate_prompt(config)\n",
    "\n",
    "# print(train[0])\n",
    "\n",
    "while True:\n",
    "    if len(train)>0:\n",
    "        prompt=train[random.choice(range(len(train)))]+'\\n'+evals[random.choice(range(len(evals)))]\n",
    "    else:\n",
    "        prompt=evals[random.choice(range(len(evals)))]\n",
    "    \n",
    "    if 'Trump' in prompt:\n",
    "        break\n",
    "\n",
    "print('\\nGenerated prompt:')\n",
    "print(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908e79b0",
   "metadata": {},
   "source": [
    "# 3. Prompt model with chain-of-thought prompting\n",
    "\n",
    "Taken from: https://gist.github.com/vgel/8a2497dc45b1ded33287fa7bb6cc1adc\n",
    "\n",
    "Code forces the model to generate at least 128 tokens before stopping. The code also used replacement tokens to bridge this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24a4ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://gist.github.com/vgel/8a2497dc45b1ded33287fa7bb6cc1adc\n",
    "\n",
    "_, _start_think_token, end_think_token = tokenizer.encode(\"<think></think>\")\n",
    "\n",
    "replacements=[\"\\nWait, but\", \"\\nHmm\", \"\\nSo\"]\n",
    "@torch.inference_mode\n",
    "def reasoning_effort(question: str, min_thinking_tokens: int):\n",
    "    tokens = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": \"<think>\\n\" + \"\"},\n",
    "        ],\n",
    "        continue_final_message=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokens = tokens.to(model.device)\n",
    "    kv = DynamicCache()\n",
    "    n_thinking_tokens = 0\n",
    "\n",
    "    print(tokenizer.decode(list(tokens[0])))\n",
    "    while True:\n",
    "        out = model(input_ids=tokens, past_key_values=kv, use_cache=True)\n",
    "        next_token = torch.multinomial(\n",
    "            torch.softmax(out.logits[0, -1, :], dim=-1), 1\n",
    "        ).item()\n",
    "        kv = out.past_key_values\n",
    "\n",
    "        if (\n",
    "            next_token in (end_think_token, model.config.eos_token_id)\n",
    "            and n_thinking_tokens < min_thinking_tokens\n",
    "        ):\n",
    "            replacement = random.choice(replacements)\n",
    "            print(replacement)\n",
    "            replacement_tokens = tokenizer.encode(replacement)\n",
    "            n_thinking_tokens += len(replacement_tokens)\n",
    "            tokens = torch.tensor([replacement_tokens]).to(tokens.device)\n",
    "        elif next_token == model.config.eos_token_id:\n",
    "            break\n",
    "        else:\n",
    "            yield tokenizer.decode([next_token])\n",
    "            n_thinking_tokens += 1\n",
    "            tokens = torch.tensor([[next_token]]).to(tokens.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d2f0b",
   "metadata": {},
   "source": [
    "## 3a. Run model with CoT prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c312a5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reasoning_effort' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[43mreasoning_effort\u001b[49m(prompt, \u001b[38;5;241m128\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(chunk, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reasoning_effort' is not defined"
     ]
    }
   ],
   "source": [
    "for chunk in reasoning_effort(prompt, 128):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b330f",
   "metadata": {},
   "source": [
    "# 3b Run model without CoT prompting\n",
    "\n",
    "This code was adapted from https://github.com/stanford-crfm/helm cited in the opinions-qa github repo to reproduce their results. It forces the model to generate only one token, which is usually one of the options presented in the prompt.\n",
    "\n",
    "Ref:https://github.com/stanford-crfm/helm/blob/main/src/helm/clients/huggingface_client.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0b66e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Question: What is your present religion, if any?\\nA. Protestant\\nB. Roman Catholic\\nC. Mormon\\nD. Orthodox\\nE. Jewish\\nF. Muslim\\nG. Buddhist\\nH. Hindu\\nI. Atheist\\nJ. Agnostic\\nK. Other\\nL. Nothing in particular\\nM. Refused\\nAnswer: F\\n\\nQuestion: Since President Trump was elected, do you think it has become more acceptable or less acceptable for people to express racist or racially insensitive views, or is it about as acceptable as it was before?\\nA. More acceptable\\nB. Less acceptable\\nC. About as acceptable\\nD. Refused\\nAnswer:',\n",
       " 'stop_sequences': [],\n",
       " 'temperature': 1e-07,\n",
       " 'max_new_tokens': 1,\n",
       " 'top_p': 1,\n",
       " 'num_return_sequences': 1,\n",
       " 'echo_prompt': False}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopping_criteria = None\n",
    "raw_request={\n",
    "    \"prompt\":prompt,\n",
    "    \"stop_sequences\": [],\n",
    "    \"temperature\":1e-7,\n",
    "    \"max_new_tokens\":1,\n",
    "    \"top_p\":1,\n",
    "    \"num_return_sequences\":1,\n",
    "    \"echo_prompt\":False\n",
    "}\n",
    "raw_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2da0fa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ss4yd/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: What is your present religion, if any?\n",
      "A. Protestant\n",
      "B. Roman Catholic\n",
      "C. Mormon\n",
      "D. Orthodox\n",
      "E. Jewish\n",
      "F. Muslim\n",
      "G. Buddhist\n",
      "H. Hindu\n",
      "I. Atheist\n",
      "J. Agnostic\n",
      "K. Other\n",
      "L. Nothing in particular\n",
      "M. Refused\n",
      "Answer: F\n",
      "\n",
      "Question: Since President Trump was elected, do you think it has become more acceptable or less acceptable for people to express racist or racially insensitive views, or is it about as acceptable as it was before?\n",
      "A. More acceptable\n",
      "B. Less acceptable\n",
      "C. About as acceptable\n",
      "D. Refused\n",
      "Answer: B\n",
      "Log Probs: [[-0.3152509331703186]]\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(raw_request[\"prompt\"], return_tensors=\"pt\", return_token_type_ids=False).to(device)\n",
    "\n",
    "output = model.generate(\n",
    "                **encoded_input,\n",
    "#                 temperature=raw_request[\"temperature\"],\n",
    "                num_return_sequences=raw_request[\"num_return_sequences\"],\n",
    "                max_new_tokens=raw_request[\"max_new_tokens\"],\n",
    "                top_p=raw_request[\"top_p\"],\n",
    "                do_sample=False,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "            )\n",
    "sequences = output.sequences\n",
    "scores = output.scores\n",
    "\n",
    "print(tokenizer.decode(sequences[0]))\n",
    "\n",
    "# calculate log probs when scores do not return inf\n",
    "if torch.isinf(scores[0]).sum().item()==0:\n",
    "\n",
    "    # Compute logprobs of generated tokens for each completed sequence.\n",
    "    all_generated_tokens_logprobs = []\n",
    "    for completion_id in range(raw_request[\"num_return_sequences\"]):\n",
    "        generated_tokens_logprobs = []\n",
    "        for i in range(len(sequences[completion_id]) - len(encoded_input.input_ids[0])):\n",
    "            logprobs = torch.nn.functional.log_softmax(scores[i][completion_id], dim=0)\n",
    "            # Get log probability of chosen token.\n",
    "            j = i + len(encoded_input.input_ids[0])\n",
    "            generated_tokens_logprobs.append(logprobs[sequences[completion_id][j]].item())\n",
    "        all_generated_tokens_logprobs.append(generated_tokens_logprobs)\n",
    "\n",
    "    print(f'Log Probs: {all_generated_tokens_logprobs}')\n",
    "else:\n",
    "    print('Log Probs not calculated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93a2e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove prompt from the start of each sequence if echo_prompt is False.\n",
    "if not raw_request[\"echo_prompt\"]:\n",
    "    sequences = [sequence[len(encoded_input.input_ids[0]) :] for sequence in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22efe29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [[tokenizer.decode(token) for token in sequence_tokens] for sequence_tokens in sequences]\n",
    "all_decoded_text = tokenizer.batch_decode(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f70c09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isinf(scores[0]).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19673901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.4.0",
   "language": "python",
   "name": "pytorch-2.4.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
