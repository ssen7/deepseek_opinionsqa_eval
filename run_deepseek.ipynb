{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e40b34bd",
   "metadata": {},
   "source": [
    "# 1. LOAD DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "619da30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13bf997df424772b0792b60adb1dfd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ref: https://gist.github.com/vgel/8a2497dc45b1ded33287fa7bb6cc1adc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache\n",
    "import torch\n",
    "\n",
    "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model_name='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('/scratch/ss4yd/huggingfacemodels/DeepSeek-R1-Distill-Qwen-7B/', torch_dtype=torch.bfloat16, device_map='auto')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aaac81",
   "metadata": {},
   "source": [
    "# 2. Generate Prompt from Opinions QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f978705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: steer-bio \n",
      "Survey file: Pew_American_Trends_Panel_disagreement_500\n",
      "10956 instances, choosing 0/0 train instances, 10956 eval instances\n",
      "\n",
      "Generated prompt:\n",
      "Question: Below you will be asked to provide a short description of your political ideology and then answer some questions.\n",
      "\n",
      "Description: I would describe my political views as Liberal.\n",
      "\n",
      "Question: Would you say the United Nations are doing a good or bad job dealing with global climate change?\n",
      "\n",
      "A. Very good job\n",
      "B. Somewhat good job\n",
      "C. Somewhat bad job\n",
      "D. Very bad job\n",
      "E. Refused\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from process_opinions_qa import generate_prompt\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "# read yaml file\n",
    "# change survey_type and context in config.yaml file for different types of prompts as described in the paper\n",
    "with open('config.yaml') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "print(f\"Context: {config['prompt']['context']} \\nSurvey file: {config['prompt']['survey_type']}\")\n",
    "train, evals=generate_prompt(config)\n",
    "\n",
    "# print(train[0])\n",
    "\n",
    "if len(train)>0:\n",
    "    prompt=train[random.choice(range(len(train)))]+'\\n'+evals[random.choice(range(len(evals)))]\n",
    "else:\n",
    "    prompt=evals[random.choice(range(len(evals)))]\n",
    "\n",
    "print('\\nGenerated prompt:')\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908e79b0",
   "metadata": {},
   "source": [
    "# 3. Prompt model with chain-of-thought prompting\n",
    "\n",
    "Taken from: https://gist.github.com/vgel/8a2497dc45b1ded33287fa7bb6cc1adc\n",
    "\n",
    "Code forces the model to generate at least 128 tokens before stopping. The code also used replacement tokens to bridge this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24a4ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://gist.github.com/vgel/8a2497dc45b1ded33287fa7bb6cc1adc\n",
    "\n",
    "_, _start_think_token, end_think_token = tokenizer.encode(\"<think></think>\")\n",
    "\n",
    "replacements=[\"\\nWait, but\", \"\\nHmm\", \"\\nSo\"]\n",
    "@torch.inference_mode\n",
    "def reasoning_effort(question: str, min_thinking_tokens: int):\n",
    "    tokens = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": \"<think>\\n\" + \"\"},\n",
    "        ],\n",
    "        continue_final_message=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokens = tokens.to(model.device)\n",
    "    kv = DynamicCache()\n",
    "    n_thinking_tokens = 0\n",
    "\n",
    "    print(tokenizer.decode(list(tokens[0])))\n",
    "    while True:\n",
    "        out = model(input_ids=tokens, past_key_values=kv, use_cache=True)\n",
    "        next_token = torch.multinomial(\n",
    "            torch.softmax(out.logits[0, -1, :], dim=-1), 1\n",
    "        ).item()\n",
    "        kv = out.past_key_values\n",
    "\n",
    "        if (\n",
    "            next_token in (end_think_token, model.config.eos_token_id)\n",
    "            and n_thinking_tokens < min_thinking_tokens\n",
    "        ):\n",
    "            replacement = random.choice(replacements)\n",
    "            print(replacement)\n",
    "            replacement_tokens = tokenizer.encode(replacement)\n",
    "            n_thinking_tokens += len(replacement_tokens)\n",
    "            tokens = torch.tensor([replacement_tokens]).to(tokens.device)\n",
    "        elif next_token == model.config.eos_token_id:\n",
    "            break\n",
    "        else:\n",
    "            yield tokenizer.decode([next_token])\n",
    "            n_thinking_tokens += 1\n",
    "            tokens = torch.tensor([[next_token]]).to(tokens.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d2f0b",
   "metadata": {},
   "source": [
    "## 3a. Run model with CoT prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c312a5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>Question: Below you will be asked to provide a short description of your political ideology and then answer some questions.\n",
      "\n",
      "Description: I would describe my political views as Liberal.\n",
      "\n",
      "Question: Would you say the United Nations are doing a good or bad job dealing with global climate change?\n",
      "\n",
      "A. Very good job\n",
      "B. Somewhat good job\n",
      "C. Somewhat bad job\n",
      "D. Very bad job\n",
      "E. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n",
      "Okay, so I need to figure out whether the United Nations are doing a good job dealing with global climate change. I think I should start by recalling what I know about the UN's role in this area. The UN has several entities, like the Intergovernmental Panel on Climate Change (IPCC), which I believe works on scientific assessments of climate change. There's also the UN Climate Change Mechanism, which I think is a framework for countries to take action on climate change.\n",
      "\n",
      "I've heard that the UN annually publishes reports, and they set targets for countries to meet, like the Paris Agreement. So, in that sense, the UN is providing some guidance. However, I also remember that climate change is a complex issue with many contributors, like burning fossil fuels, deforestation, and industrial activities. I think the issue isn't just something the UN can fully control, but it's also influenced by national policies and international law.\n",
      "\n",
      "I've seen countries around the world, both developed and developing, adapting to climate change. Some have taken significant actions, while others are still lagging. It seems like the effectiveness of the UN's climate efforts can vary depending on member countries' willingness to cooperate. There might be times when the United Nations is effective, but other times, maybe not as much as expected if certain countries don't push harder.\n",
      "\n",
      "Also, funding for climate action is often a part of these efforts, and I think some European countries and 输入国家 have contributed more to the climate, but others have relied on international aid. The adaptability and responsiveness of the UN system might help in some areas but have limitations based on member volition and political will.\n",
      "\n",
      "I should weigh the pros and cons. On one hand, the UN initiates international agreements and sets standards that countries strive to meet. On the other hand, the global issue is vast, so relying solely on the UN might not cover all aspects, and member nations' implementations can vary widely.\n",
      "\n",
      "Putting it all together, I think the UN is doing a somewhat good job because they have institutions and frameworks in place, but the actual effectiveness depends on member countries' commitment. Maybe it's not the best, but it's better than some other organizations or not having any action at all.\n",
      "</think>\n",
      "\n",
      "The United Nations is functioning as a somewhat good job in addressing global climate change. While the UN provides essential frameworks, conventions, and guidelines through institutions like the IPCC and the Climate Mechanism, the effectiveness can vary based on member countries' commitment and political will. Opportunities exist for stronger action, but the UN has demonstrated competence in establishing international standards and targets."
     ]
    }
   ],
   "source": [
    "for chunk in reasoning_effort(prompt, 128):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b330f",
   "metadata": {},
   "source": [
    "# 3b Run model without CoT prompting\n",
    "\n",
    "This code was adapted from https://github.com/stanford-crfm/helm cited in the opinions-qa github repo to reproduce their results. It forces the model to generate only one token, which is usually one of the options presented in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da0fa01",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m stopping_criteria \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      2\u001b[0m raw_request\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[43mprompt\u001b[49m,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m1e-7\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mecho_prompt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     12\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m tokenizer(raw_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_token_type_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     15\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoded_input,\n\u001b[1;32m     16\u001b[0m                 temperature\u001b[38;5;241m=\u001b[39mraw_request[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m     28\u001b[0m             )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "stopping_criteria = None\n",
    "raw_request={\n",
    "    \"prompt\":prompt,\n",
    "    \"stop_sequences\": [],\n",
    "    \"temperature\":1e-7,\n",
    "    \"max_new_tokens\":1,\n",
    "    \"top_p\":1,\n",
    "    \"num_return_sequences\":1,\n",
    "    \"echo_prompt\":False\n",
    "}\n",
    "\n",
    "encoded_input = tokenizer(raw_request[\"prompt\"], return_tensors=\"pt\", return_token_type_ids=False).to(device)\n",
    "\n",
    "output = model.generate(\n",
    "                **encoded_input,\n",
    "                temperature=raw_request[\"temperature\"],\n",
    "#                 temperature=None,\n",
    "                num_return_sequences=raw_request[\"num_return_sequences\"],\n",
    "                max_new_tokens=raw_request[\"max_new_tokens\"],\n",
    "                top_p=raw_request[\"top_p\"],\n",
    "#                 do_sample=False,\n",
    "                do_sample=True,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                output_logits=True,\n",
    "#                 **optional_args,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "            )\n",
    "sequences = output.sequences\n",
    "scores = output.logits\n",
    "\n",
    "print(tokenizer.decode(sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2123d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2e8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22efe29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70c09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19673901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.4.0",
   "language": "python",
   "name": "pytorch-2.4.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
