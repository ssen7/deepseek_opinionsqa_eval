{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e40b34bd",
   "metadata": {},
   "source": [
    "# 1. LOAD DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "619da30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6eff1c3c0af4edd9c5d3715d1e7c664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ref: https://gist.github.com/vgel/8a2497dc45b1ded33287fa7bb6cc1adc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache\n",
    "import torch\n",
    "\n",
    "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model_name='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('/scratch/ss4yd/huggingfacemodels/DeepSeek-R1-Distill-Qwen-7B/', \n",
    "                                             torch_dtype=torch.bfloat16, device_map='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aaac81",
   "metadata": {},
   "source": [
    "# 2. Generate Prompt from Opinions QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f978705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: steer-bio \n",
      "Survey file: Pew_American_Trends_Panel_disagreement_500\n",
      "\n",
      "Generated prompt:\n",
      "Question: What sex were you assigned at birth?\n",
      "A. Male\n",
      "B. Female\n",
      "C. Refused\n",
      "Answer: A\n",
      "\n",
      "Question: In general, would you say your view of environmental research scientists is\n",
      "A. Mostly positive\n",
      "B. Mostly negative\n",
      "C. Neither positive nor negative\n",
      "D. Refused\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from process_opinions_qa import generate_prompt\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "# read yaml file\n",
    "# change survey_type and context in config.yaml file for different types of prompts as described in the paper\n",
    "with open('config.yaml') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "print(f\"Context: {config['prompt']['context']} \\nSurvey file: {config['prompt']['survey_type']}\")\n",
    "\n",
    "manual=True # use config file if False, set manually if True\n",
    "if manual:\n",
    "    config=dict()\n",
    "    config['prompt']=dict()\n",
    "    config['prompt']['context']='steer-qa' # values: default, steer-qa, steer-bio, steer-portray\n",
    "    config['prompt']['survey_type']='Pew_American_Trends_Panel_disagreement_500' # values: Pew_American_Trends_Panel_W26, replace 26 with the [27,27,29,32..] etc.\n",
    "    config['prompt']['output_path']='./'\n",
    "    config['prompt']['include_output']=False\n",
    "    config['prompt']['reference_index']=None\n",
    "    \n",
    "train, evals=generate_prompt(config)\n",
    "\n",
    "# print(train[0])\n",
    "\n",
    "if len(train)>0:\n",
    "    prompt=train[random.choice(range(len(train)))]+'\\n'+evals[random.choice(range(len(evals)))]\n",
    "else:\n",
    "    prompt=evals[random.choice(range(len(evals)))]\n",
    "\n",
    "print('\\nGenerated prompt:')\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908e79b0",
   "metadata": {},
   "source": [
    "# 3. Prompt model with chain-of-thought prompting\n",
    "\n",
    "Taken from: https://gist.github.com/vgel/8a2497dc45b1ded33287fa7bb6cc1adc\n",
    "\n",
    "Code forces the model to generate at least 128 tokens before stopping. The code also used replacement tokens to bridge this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24a4ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://gist.github.com/vgel/8a2497dc45b1ded33287fa7bb6cc1adc\n",
    "\n",
    "_, _start_think_token, end_think_token = tokenizer.encode(\"<think></think>\")\n",
    "\n",
    "replacements=[\"\\nWait, but\", \"\\nHmm\", \"\\nSo\"]\n",
    "@torch.inference_mode\n",
    "def reasoning_effort(question: str, min_thinking_tokens: int):\n",
    "    tokens = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": \"<think>\\n\" + \"\"},\n",
    "        ],\n",
    "        continue_final_message=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokens = tokens.to(model.device)\n",
    "    kv = DynamicCache()\n",
    "    n_thinking_tokens = 0\n",
    "\n",
    "    print(tokenizer.decode(list(tokens[0])))\n",
    "    while True:\n",
    "        out = model(input_ids=tokens, past_key_values=kv, use_cache=True)\n",
    "        next_token = torch.multinomial(\n",
    "            torch.softmax(out.logits[0, -1, :], dim=-1), 1\n",
    "        ).item()\n",
    "        kv = out.past_key_values\n",
    "\n",
    "        if (\n",
    "            next_token in (end_think_token, model.config.eos_token_id)\n",
    "            and n_thinking_tokens < min_thinking_tokens\n",
    "        ):\n",
    "            replacement = random.choice(replacements)\n",
    "            print(replacement)\n",
    "            replacement_tokens = tokenizer.encode(replacement)\n",
    "            n_thinking_tokens += len(replacement_tokens)\n",
    "            tokens = torch.tensor([replacement_tokens]).to(tokens.device)\n",
    "        elif next_token == model.config.eos_token_id:\n",
    "            break\n",
    "        else:\n",
    "            yield tokenizer.decode([next_token])\n",
    "            n_thinking_tokens += 1\n",
    "            tokens = torch.tensor([[next_token]]).to(tokens.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d2f0b",
   "metadata": {},
   "source": [
    "## 3a. Run model with CoT prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c312a5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>Question: What sex were you assigned at birth?\n",
      "A. Male\n",
      "B. Female\n",
      "C. Refused\n",
      "Answer: A\n",
      "\n",
      "Question: In general, would you say your view of environmental research scientists is\n",
      "A. Mostly positive\n",
      "B. Mostly negative\n",
      "C. Neither positive nor negative\n",
      "D. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n",
      "Okay, so the user provided a question about environmental research scientists' general view and asked for an answer. Looking at the previous interaction, the assistant chose option C, \"Neither positive nor negative.\" \n",
      "\n",
      "Hmm, the user has a history of asking about gender and then a question about a specific group's perception. This time, it's about environmental research scientists. The options are mostly positive, mostly negative, neither, or refused.\n",
      "\n",
      "I should consider what factors influence someone's view of environmental scientists. They might value their work since it contributes to sustainability and mitigating climate change. Many people appreciate their efforts in combating environmental issues. \n",
      "\n",
      "On the other hand, sometimes science can have unintended consequences, like environmental disasters from industrial activities. So, some might have a negative view, but it's probably not the majority since recognizing the positive impact is likely more common.\n",
      "\n",
      "Looking at similar questions before, the answer was \"Neither,\" so maybe this follows the same trend. People have mixed feelings but the overall sentiment isn't purely positive or negative. \n",
      "\n",
      "I need to think if there's more nuance. Perhaps the view could be more leaning towards positive, but the answer given is \"Neither,\" which suggests a balanced perspective. That makes sense because while their work is important, there are critics who see scientific methods as limited or focused on short-term gains.\n",
      "\n",
      "So, the answer should support the reasoning that while their work is valuable, there are criticisms that lead to a \"Neither positive nor negative\" response.\n",
      "</think>\n",
      "\n",
      "The answer is **C. Neither positive nor negative**. \n",
      "\n",
      "Environmental research scientists are often highly regarded for their work in addressing environmental challenges, but some may criticize the limitations of scientific approaches or the focus on short-term fixes. Therefore, a balanced view is possible, making \"Neither positive nor negative\" a reasonable choice."
     ]
    }
   ],
   "source": [
    "for chunk in reasoning_effort(prompt, 128):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b330f",
   "metadata": {},
   "source": [
    "# 3b Run model without CoT prompting\n",
    "\n",
    "This code was adapted from https://github.com/stanford-crfm/helm cited in the opinions-qa github repo to reproduce their results. It forces the model to generate only one token, which is usually one of the options presented in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2da0fa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: What sex were you assigned at birth?\n",
      "A. Male\n",
      "B. Female\n",
      "C. Refused\n",
      "Answer: A\n",
      "\n",
      "Question: In general, would you say your view of environmental research scientists is\n",
      "A. Mostly positive\n",
      "B. Mostly negative\n",
      "C. Neither positive nor negative\n",
      "D. Refused\n",
      "Answer: A\n"
     ]
    }
   ],
   "source": [
    "stopping_criteria = None\n",
    "raw_request={\n",
    "    \"prompt\":prompt,\n",
    "    \"stop_sequences\": [],\n",
    "    \"temperature\":1e-7,\n",
    "    \"max_new_tokens\":1,\n",
    "    \"top_p\":1,\n",
    "    \"num_return_sequences\":1,\n",
    "    \"echo_prompt\":False\n",
    "}\n",
    "\n",
    "encoded_input = tokenizer(raw_request[\"prompt\"], return_tensors=\"pt\", return_token_type_ids=False).to(device)\n",
    "\n",
    "output = model.generate(\n",
    "                **encoded_input,\n",
    "                temperature=raw_request[\"temperature\"],\n",
    "#                 temperature=None,\n",
    "                num_return_sequences=raw_request[\"num_return_sequences\"],\n",
    "                max_new_tokens=raw_request[\"max_new_tokens\"],\n",
    "                top_p=raw_request[\"top_p\"],\n",
    "#                 do_sample=False,\n",
    "                do_sample=True,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                output_logits=True,\n",
    "#                 **optional_args,\n",
    "                stopping_criteria=stopping_criteria,\n",
    "            )\n",
    "sequences = output.sequences\n",
    "scores = output.logits\n",
    "\n",
    "print(tokenizer.decode(sequences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2123d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a2e8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22efe29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70c09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19673901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.4.0",
   "language": "python",
   "name": "pytorch-2.4.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
