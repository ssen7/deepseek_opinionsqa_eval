{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e40b34bd",
   "metadata": {},
   "source": [
    "# 1. LOAD DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "619da30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca845942b92489197d285862c56adc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ref: https://gist.github.com/vgel/8a2497dc45b1ded33287fa7bb6cc1adc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache\n",
    "import torch\n",
    "\n",
    "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model_name='deepseek-ai/DeepSeek-R1-Distill-Qwen-32B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('/scratch/ss4yd/huggingfacemodels/DeepSeek-R1-Distill-Qwen-32B/', \n",
    "                                             torch_dtype=torch.bfloat16, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efdde8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>options</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please think about what things will be like in...</td>\n",
       "      <td>['Very optimistic', 'Somewhat optimistic', \\r\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Over the next 30 years, do you think that the ...</td>\n",
       "      <td>['Get better', 'Stay about the same', \\r\\n'Get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thinking about how Chinese society sees men th...</td>\n",
       "      <td>['Mostpeoplelook uptomenwhoaremanlyormasculine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Over the next 30 years, do you think that crim...</td>\n",
       "      <td>['Get better', 'Stay about the same', \\r\\n'Get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Over the next 30 years, do you think that yout...</td>\n",
       "      <td>['Get better', 'Stay about the same', \\r\\n'Get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Over the next 30 years, do you think that Chin...</td>\n",
       "      <td>['Get better', 'Stay about the same', \\r\\n'Get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Over the next 30 years, do you think that Chin...</td>\n",
       "      <td>['Get better', 'Stay about the same', \\r\\n'Get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Over the next 30 years, do you think that soci...</td>\n",
       "      <td>['Get better', 'Stay about the same', \\r\\n'Get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Do you think China's current economic conditio...</td>\n",
       "      <td>['Helping a lot','Helping a little', 'Hurting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Do you think China's current economic conditio...</td>\n",
       "      <td>['Helping a lot','Helping a little', 'Hurting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Do you think China's current economic conditio...</td>\n",
       "      <td>['Helping a lot','Helping a little', 'Hurting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Do you think China's current economic conditio...</td>\n",
       "      <td>['Helping a lot','Helping a little', 'Hurting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>When it comes to whether or not to limit Chine...</td>\n",
       "      <td>['Strongly support limiting Chinese students',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>When it comes to whether or not to limit Ameri...</td>\n",
       "      <td>['Strongly support limiting American students'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Do you think independent civil society is gene...</td>\n",
       "      <td>['Very good for society','Somewhat good for so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Do you think protests are generally good or ba...</td>\n",
       "      <td>['Very good for society','Somewhat good for so...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   Please think about what things will be like in...   \n",
       "1   Over the next 30 years, do you think that the ...   \n",
       "2   Thinking about how Chinese society sees men th...   \n",
       "3   Over the next 30 years, do you think that crim...   \n",
       "4   Over the next 30 years, do you think that yout...   \n",
       "5   Over the next 30 years, do you think that Chin...   \n",
       "6   Over the next 30 years, do you think that Chin...   \n",
       "7   Over the next 30 years, do you think that soci...   \n",
       "8   Do you think China's current economic conditio...   \n",
       "9   Do you think China's current economic conditio...   \n",
       "10  Do you think China's current economic conditio...   \n",
       "11  Do you think China's current economic conditio...   \n",
       "12  When it comes to whether or not to limit Chine...   \n",
       "13  When it comes to whether or not to limit Ameri...   \n",
       "14  Do you think independent civil society is gene...   \n",
       "15  Do you think protests are generally good or ba...   \n",
       "\n",
       "                                              options  \n",
       "0   ['Very optimistic', 'Somewhat optimistic', \\r\\...  \n",
       "1   ['Get better', 'Stay about the same', \\r\\n'Get...  \n",
       "2   ['Mostpeoplelook uptomenwhoaremanlyormasculine...  \n",
       "3   ['Get better', 'Stay about the same', \\r\\n'Get...  \n",
       "4   ['Get better', 'Stay about the same', \\r\\n'Get...  \n",
       "5   ['Get better', 'Stay about the same', \\r\\n'Get...  \n",
       "6   ['Get better', 'Stay about the same', \\r\\n'Get...  \n",
       "7   ['Get better', 'Stay about the same', \\r\\n'Get...  \n",
       "8   ['Helping a lot','Helping a little', 'Hurting ...  \n",
       "9   ['Helping a lot','Helping a little', 'Hurting ...  \n",
       "10  ['Helping a lot','Helping a little', 'Hurting ...  \n",
       "11  ['Helping a lot','Helping a little', 'Hurting ...  \n",
       "12  ['Strongly support limiting Chinese students',...  \n",
       "13  ['Strongly support limiting American students'...  \n",
       "14  ['Very good for society','Somewhat good for so...  \n",
       "15  ['Very good for society','Somewhat good for so...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('./data/model_input/UVA_Data.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aaac81",
   "metadata": {},
   "source": [
    "# 2. Generate Prompt from Opinions QA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f978705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: default \n",
      "Survey file: UVA_Data\n",
      "\n",
      "Generated prompt:\n",
      "Question: Over the next 30 years, do you think that social stability in China will\n",
      "A. Get better\n",
      "B. Stay about the same\n",
      "C. Get worse\n",
      "D. Refused\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from process_opinions_qa import generate_prompt\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "# read yaml file\n",
    "# change survey_type and context in config.yaml file for different types of prompts as described in the paper\n",
    "with open('config.yaml') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "manual=True # use config file if False, set manually if True\n",
    "if manual:\n",
    "    config=dict()\n",
    "    config['prompt']=dict()\n",
    "    config['prompt']['context']='default' # values: default, steer-qa, steer-bio, steer-portray\n",
    "    config['prompt']['survey_type']='UVA_Data' # values: Pew_American_Trends_Panel_disagreement_500,Pew_American_Trends_Panel_W26, replace 26 with the [26,27,29,32..] etc.\n",
    "    config['prompt']['output_path']='./'\n",
    "    config['prompt']['include_output']=False\n",
    "    config['prompt']['reference_index']=None\n",
    "\n",
    "print(f\"Context: {config['prompt']['context']} \\nSurvey file: {config['prompt']['survey_type']}\")\n",
    "\n",
    "train, evals=generate_prompt(config)\n",
    "\n",
    "# print(train[0])\n",
    "\n",
    "# while True:\n",
    "if len(train)>0:\n",
    "    prompt=train[random.choice(range(len(train)))]+'\\n'+evals[random.choice(range(len(evals)))]\n",
    "else:\n",
    "    prompt=evals[random.choice(range(len(evals)))]\n",
    "\n",
    "#     if 'Xi' in prompt:\n",
    "#         break\n",
    "    \n",
    "\n",
    "print('\\nGenerated prompt:')\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908e79b0",
   "metadata": {},
   "source": [
    "# 3. Prompt model with chain-of-thought prompting\n",
    "\n",
    "Taken from: https://gist.github.com/vgel/8a2497dc45b1ded33287fa7bb6cc1adc\n",
    "\n",
    "Code forces the model to generate at least 128 tokens before stopping. The code also used replacement tokens to bridge this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24a4ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://gist.github.com/vgel/8a2497dc45b1ded33287fa7bb6cc1adc\n",
    "\n",
    "_, _start_think_token, end_think_token = tokenizer.encode(\"<think></think>\")\n",
    "\n",
    "\n",
    "replacements=[\"\\nWait, but\", \"\\nHmm\", \"\\nSo\"]\n",
    "@torch.inference_mode\n",
    "def reasoning_effort(question: str, min_thinking_tokens: int):\n",
    "    tokens = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": \"<think>\\n\" + \"\"},\n",
    "        ],\n",
    "        continue_final_message=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    tokens = tokens.to(model.device)\n",
    "    kv = DynamicCache()\n",
    "    n_thinking_tokens = 0\n",
    "\n",
    "    print(tokenizer.decode(list(tokens[0])))\n",
    "    while True:\n",
    "        out = model(input_ids=tokens, past_key_values=kv, use_cache=True)\n",
    "        next_token = torch.multinomial(\n",
    "            torch.softmax(out.logits[0, -1, :], dim=-1), 1\n",
    "        ).item()\n",
    "        kv = out.past_key_values\n",
    "\n",
    "        if (\n",
    "            next_token in (end_think_token, model.config.eos_token_id)\n",
    "            and n_thinking_tokens < min_thinking_tokens\n",
    "        ):\n",
    "            replacement = random.choice(replacements)\n",
    "            print(replacement)\n",
    "            replacement_tokens = tokenizer.encode(replacement)\n",
    "            n_thinking_tokens += len(replacement_tokens)\n",
    "            tokens = torch.tensor([replacement_tokens]).to(tokens.device)\n",
    "        elif next_token == model.config.eos_token_id:\n",
    "            break\n",
    "        else:\n",
    "            dec_token = tokenizer.decode([next_token])\n",
    "            yield tokenizer.decode([next_token])\n",
    "            n_thinking_tokens += 1\n",
    "            tokens = torch.tensor([[next_token]]).to(tokens.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d2f0b",
   "metadata": {},
   "source": [
    "## 3a. Run model with CoT prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c312a5f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_reasoning_response(prompt, if_print=True):\n",
    "    think_responses=list()\n",
    "    for chunk in reasoning_effort(prompt, 128):\n",
    "        if if_print:\n",
    "            print(chunk, end=\"\", flush=True)\n",
    "        think_responses.append(chunk)\n",
    "    return ''.join(think_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b330f",
   "metadata": {},
   "source": [
    "# 3b Run model without CoT prompting\n",
    "\n",
    "This code was adapted from https://github.com/stanford-crfm/helm cited in the opinions-qa github repo to reproduce their results. It forces the model to generate only one token, which is usually one of the options presented in the prompt.\n",
    "\n",
    "Ref:https://github.com/stanford-crfm/helm/blob/main/src/helm/clients/huggingface_client.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc80478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_logprob_dictionary(logprobs, tokenizer, k=10):\n",
    "    res_dict=dict()\n",
    "    topklogprobs=logprobs.topk(k=k)\n",
    "    logprob_vals=topklogprobs.values\n",
    "    logprob_inds=topklogprobs.indices\n",
    "    \n",
    "    res_dict['text']=tokenizer.decode([logprob_inds[0].item()])\n",
    "    res_dict['logprob']=logprob_vals[0].item()\n",
    "    res_dict['top_logprobs']=dict()\n",
    "    for i in range(k):\n",
    "        res_dict['top_logprobs'][tokenizer.decode([logprob_inds[i].item()])]= \\\n",
    "            logprob_vals[i].item()\n",
    "        \n",
    "    return res_dict\n",
    "\n",
    "def generate_responses_logprobs(prompt, model, tokenizer):\n",
    "    stopping_criteria = None\n",
    "    raw_request={\n",
    "        \"prompt\":prompt,\n",
    "        \"stop_sequences\": [],\n",
    "        \"temperature\":1e-7,\n",
    "        \"max_new_tokens\":1,\n",
    "        \"top_p\":1,\n",
    "        \"num_return_sequences\":1,\n",
    "        \"echo_prompt\":False,\n",
    "    }\n",
    "    \n",
    "    encoded_input = tokenizer(raw_request[\"prompt\"], return_tensors=\"pt\", return_token_type_ids=False).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "                        **encoded_input,\n",
    "        #                 temperature=raw_request[\"temperature\"],\n",
    "                        num_return_sequences=raw_request[\"num_return_sequences\"],\n",
    "                        max_new_tokens=raw_request[\"max_new_tokens\"],\n",
    "                        top_p=raw_request[\"top_p\"],\n",
    "                        do_sample=False,\n",
    "                        return_dict_in_generate=True,\n",
    "                        output_scores=True,\n",
    "                        stopping_criteria=stopping_criteria,\n",
    "        #                 top_k=raw_request[\"top_k\"],\n",
    "                    )\n",
    "    sequences = output.sequences\n",
    "    scores = output.scores\n",
    "\n",
    "    print(tokenizer.decode(sequences[0]))\n",
    "\n",
    "    # calculate log probs when scores do not return inf\n",
    "    if torch.isinf(scores[0]).sum().item()==0:\n",
    "\n",
    "        # Compute logprobs of generated tokens for each completed sequence.\n",
    "        all_generated_tokens_logprobs = []\n",
    "        for completion_id in range(raw_request[\"num_return_sequences\"]):\n",
    "    #         print(f'completion id: {completion_id}')\n",
    "            generated_tokens_logprobs = []\n",
    "            for i in range(len(sequences[completion_id]) - len(encoded_input.input_ids[0])):\n",
    "    #             print(f'i: {i}')\n",
    "                logprobs = torch.nn.functional.log_softmax(scores[i][completion_id], dim=0)\n",
    "    #             print(f'logprobs: {len(logprobs)}')\n",
    "                # Get log probability of chosen token.\n",
    "                j = i + len(encoded_input.input_ids[0])\n",
    "    #             print(f'j: {j}')\n",
    "                generated_tokens_logprobs.append(logprobs[sequences[completion_id][j]].item())\n",
    "            all_generated_tokens_logprobs.append(generated_tokens_logprobs)\n",
    "\n",
    "        print(f'Log Probs: {all_generated_tokens_logprobs}')\n",
    "    else:\n",
    "        print('Log Probs not calculated')\n",
    "        \n",
    "    \n",
    "    res_dict=generate_logprob_dictionary(logprobs, tokenizer)\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e49b0b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b817edd147dc426d8ca8abb63ad9d08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>Question: Please think about what things will be like in 2050, about 30 years from now. \r\n",
      "Thinking about the future of the People's Republic of China, would you say you are\n",
      "A. Very optimistic\n",
      "B. Somewhat optimistic\n",
      "C. Somewhat pessimistic\n",
      "D. Very pessimistic\n",
      "E. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: Please think about what things will be like in 2050, about 30 years from now. \r\n",
      "Thinking about the future of the People's Republic of China, would you say you are\n",
      "A. Very optimistic\n",
      "B. Somewhat optimistic\n",
      "C. Somewhat pessimistic\n",
      "D. Very pessimistic\n",
      "E. Refused\n",
      "Answer: A\n",
      "Log Probs: [[-0.8920373320579529]]\n",
      "<｜begin▁of▁sentence｜><｜User｜>Question: Over the next 30 years, do you think that the average Chinese family will see \r\n",
      "its standard of living\n",
      "A. Get better\n",
      "B. Stay about the same\n",
      "C. Get worse\n",
      "D. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: Over the next 30 years, do you think that the average Chinese family will see \r\n",
      "its standard of living\n",
      "A. Get better\n",
      "B. Stay about the same\n",
      "C. Get worse\n",
      "D. Refused\n",
      "Answer: A\n",
      "Log Probs: [[-0.6002359390258789]]\n",
      "<｜begin▁of▁sentence｜><｜User｜>Question: Thinking about how Chinese society sees men these days, in general, would you say\n",
      "A. Mostpeoplelook uptomenwhoaremanlyormasculine\n",
      "B. Most people look down on men who are manly or masculine\n",
      "C. Neither\n",
      "D. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: Thinking about how Chinese society sees men these days, in general, would you say\n",
      "A. Mostpeoplelook uptomenwhoaremanlyormasculine\n",
      "B. Most people look down on men who are manly or masculine\n",
      "C. Neither\n",
      "D. Refused\n",
      "Answer: A\n",
      "Log Probs: [[-0.5656664967536926]]\n",
      "<｜begin▁of▁sentence｜><｜User｜>Question: Over the next 30 years, do you think that crime rates in Chinese cities will \n",
      "A. Get better\n",
      "B. Stay about the same\n",
      "C. Get worse\n",
      "D. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: Over the next 30 years, do you think that crime rates in Chinese cities will \n",
      "A. Get better\n",
      "B. Stay about the same\n",
      "C. Get worse\n",
      "D. Refused\n",
      "Answer: A\n",
      "Log Probs: [[-0.734650194644928]]\n",
      "<｜begin▁of▁sentence｜><｜User｜>Question: Over the next 30 years, do you think that youth unemployment in China will\n",
      "A. Get better\n",
      "B. Stay about the same\n",
      "C. Get worse\n",
      "D. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: Over the next 30 years, do you think that youth unemployment in China will\n",
      "A. Get better\n",
      "B. Stay about the same\n",
      "C. Get worse\n",
      "D. Refused\n",
      "Answer: A\n",
      "Log Probs: [[-0.6193665862083435]]\n",
      "<｜begin▁of▁sentence｜><｜User｜>Question: Over the next 30 years, do you think that Chinese demographic problems will\n",
      "A. Get better\n",
      "B. Stay about the same\n",
      "C. Get worse\n",
      "D. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: Over the next 30 years, do you think that Chinese demographic problems will\n",
      "A. Get better\n",
      "B. Stay about the same\n",
      "C. Get worse\n",
      "D. Refused\n",
      "Answer: C\n",
      "Log Probs: [[-0.5558454990386963]]\n",
      "<｜begin▁of▁sentence｜><｜User｜>Question: Over the next 30 years, do you think that Chinese birthrates will \n",
      "A. Get better\n",
      "B. Stay about the same\n",
      "C. Get worse\n",
      "D. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: Over the next 30 years, do you think that Chinese birthrates will \n",
      "A. Get better\n",
      "B. Stay about the same\n",
      "C. Get worse\n",
      "D. Refused\n",
      "Answer: C\n",
      "Log Probs: [[-1.0038158893585205]]\n",
      "<｜begin▁of▁sentence｜><｜User｜>Question: Over the next 30 years, do you think that social stability in China will\n",
      "A. Get better\n",
      "B. Stay about the same\n",
      "C. Get worse\n",
      "D. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: Over the next 30 years, do you think that social stability in China will\n",
      "A. Get better\n",
      "B. Stay about the same\n",
      "C. Get worse\n",
      "D. Refused\n",
      "Answer: A\n",
      "Log Probs: [[-1.1590733528137207]]\n",
      "<｜begin▁of▁sentence｜><｜User｜>Question: Do you think China's current economic conditions are helping or hurting Han people?\n",
      "A. Helping a lot\n",
      "B. Helping a little\n",
      "C. Hurting a little\n",
      "D. Hurting a lot\n",
      "E. Neither helping nor hurting\n",
      "F. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: Do you think China's current economic conditions are helping or hurting Han people?\n",
      "A. Helping a lot\n",
      "B. Helping a little\n",
      "C. Hurting a little\n",
      "D. Hurting a lot\n",
      "E. Neither helping nor hurting\n",
      "F. Refused\n",
      "Answer: E\n",
      "Log Probs: [[-1.357311487197876]]\n",
      "<｜begin▁of▁sentence｜><｜User｜>Question: Do you think China's current economic conditions are helping or hurting Uyghur people?\n",
      "A. Helping a lot\n",
      "B. Helping a little\n",
      "C. Hurting a little\n",
      "D. Hurting a lot\n",
      "E. Neither helping nor hurting\n",
      "F. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: Do you think China's current economic conditions are helping or hurting Uyghur people?\n",
      "A. Helping a lot\n",
      "B. Helping a little\n",
      "C. Hurting a little\n",
      "D. Hurting a lot\n",
      "E. Neither helping nor hurting\n",
      "F. Refused\n",
      "Answer: E\n",
      "Log Probs: [[-1.5869132280349731]]\n",
      "<｜begin▁of▁sentence｜><｜User｜>Question: Do you think China's current economic conditions are helping or hurting Tibetan people?\n",
      "A. Helping a lot\n",
      "B. Helping a little\n",
      "C. Hurting a little\n",
      "D. Hurting a lot\n",
      "E. Neither helping nor hurting\n",
      "F. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: Do you think China's current economic conditions are helping or hurting Tibetan people?\n",
      "A. Helping a lot\n",
      "B. Helping a little\n",
      "C. Hurting a little\n",
      "D. Hurting a lot\n",
      "E. Neither helping nor hurting\n",
      "F. Refused\n",
      "Answer: A\n",
      "Log Probs: [[-0.8936105966567993]]\n",
      "<｜begin▁of▁sentence｜><｜User｜>Question: Do you think China's current economic conditions are helping or hurting Hui people?\n",
      "A. Helping a lot\n",
      "B. Helping a little\n",
      "C. Hurting a little\n",
      "D. Hurting a lot\n",
      "E. Neither helping nor hurting\n",
      "F. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: Do you think China's current economic conditions are helping or hurting Hui people?\n",
      "A. Helping a lot\n",
      "B. Helping a little\n",
      "C. Hurting a little\n",
      "D. Hurting a lot\n",
      "E. Neither helping nor hurting\n",
      "F. Refused\n",
      "Answer: E\n",
      "Log Probs: [[-1.3398102521896362]]\n",
      "<｜begin▁of▁sentence｜><｜User｜>Question: When it comes to whether or not to limit Chinese students studying in the U.S., do you \n",
      "A. Strongly support limiting Chinese students\n",
      "B. Somewhat support limiting Chinese students\n",
      "C. Somewhat oppose limiting Chinese students\n",
      "D. Strongly oppose limiting Chinese students\n",
      "E. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: When it comes to whether or not to limit Chinese students studying in the U.S., do you \n",
      "A. Strongly support limiting Chinese students\n",
      "B. Somewhat support limiting Chinese students\n",
      "C. Somewhat oppose limiting Chinese students\n",
      "D. Strongly oppose limiting Chinese students\n",
      "E. Refused\n",
      "Answer: D\n",
      "Log Probs: [[-0.9192594885826111]]\n",
      "<｜begin▁of▁sentence｜><｜User｜>Question: When it comes to whether or not to limit American students studying in China, do you \n",
      "A. Strongly support limiting American students\n",
      "B. Somewhat support limiting American students\n",
      "C. Somewhat oppose limiting American students\n",
      "D. Strongly oppose limiting American students\n",
      "E. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: When it comes to whether or not to limit American students studying in China, do you \n",
      "A. Strongly support limiting American students\n",
      "B. Somewhat support limiting American students\n",
      "C. Somewhat oppose limiting American students\n",
      "D. Strongly oppose limiting American students\n",
      "E. Refused\n",
      "Answer: D\n",
      "Log Probs: [[-1.0103249549865723]]\n",
      "<｜begin▁of▁sentence｜><｜User｜>Question: Do you think independent civil society is generally good or bad for our country?\n",
      "A. Very good for society\n",
      "B. Somewhat good for society\n",
      "C. Neither good nor bad for society\n",
      "D. Somewhat bad for society\n",
      "E. Very bad for society\n",
      "F. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: Do you think independent civil society is generally good or bad for our country?\n",
      "A. Very good for society\n",
      "B. Somewhat good for society\n",
      "C. Neither good nor bad for society\n",
      "D. Somewhat bad for society\n",
      "E. Very bad for society\n",
      "F. Refused\n",
      "Answer: A\n",
      "Log Probs: [[-0.8360346555709839]]\n",
      "<｜begin▁of▁sentence｜><｜User｜>Question: Do you think protests are generally good or bad for our society?\n",
      "A. Very good for society\n",
      "B. Somewhat good for society\n",
      "C. Neither good nor bad for society\n",
      "D. Somewhat bad for society\n",
      "E. Very bad for society\n",
      "F. Refused\n",
      "Answer:<｜Assistant｜><think>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>Question: Do you think protests are generally good or bad for our society?\n",
      "A. Very good for society\n",
      "B. Somewhat good for society\n",
      "C. Neither good nor bad for society\n",
      "D. Somewhat bad for society\n",
      "E. Very bad for society\n",
      "F. Refused\n",
      "Answer: B\n",
      "Log Probs: [[-1.3965107202529907]]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "reasoning_responses=list()\n",
    "reasoning_logprobs=list()\n",
    "for prompt in tqdm(evals):\n",
    "    reasoning_responses.append(get_reasoning_response(prompt, if_print=False))\n",
    "    reasoning_logprobs.append(generate_responses_logprobs(prompt, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c95eb76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "df['question']=evals\n",
    "df['reasoning_responses']=reasoning_responses\n",
    "df['top_k_logprobs']=reasoning_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62250c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('DeepSeek_R1_Qwen32B_distill_UVA_question_responses.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afdc927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('DeepSeek_R1_Qwen32B_distill_UVA_question_responses.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
